## 2.2 词（*Words*）

在我们谈论处理词之前，我们需要先确认下到底什么算作一个单词。让我们先看一个具体的语料库（*corpus*）（复数 corpora），一个计算机可读的文本或语音集合。例如 Brown 语料库（*Brown corpus*）是布朗大学在 1963-64 年间（Kucera and Francis, 1967）[^1]收集的 500 篇不同体裁（报纸、小说、非小说、学术等）的英语书面文本样本集合，共有约 100 万个词。下面来自该语料库的句子中有多少个词？

> He stepped out into the hall, was delighted to encounter a water brother.

如果我们不把标点符号算作词，那么这个句子就有 13 个词，否则就有 15 个。我们是否把句号（“.”）、逗号（“,”）等当作词，要视任务而定。标点符号对于寻找边界（逗号、句号、冒号）和分析含义（问号、感叹号、引号）至关重要。对于某些任务，如词性标注（*part-of-speech tagging*）或解析（*parsing*）或语音合成（*speech synthesis*），我们有时会把标点符号当作单独的词来处理。

Switchboard 语料库是在 20 世纪 90 年代初收集到的美国陌生人之间的英语电话对话的集合，它包含了 2430 次平均每次 6 分钟的对话，共计 240 个小时的语音和约 300 万个单词（Godfrey et al., 1992）[^2]。这样的口语化文本没有标点符号，但在定义词语方面却带来了更复杂的问题。我们来看看 Switchboard 中的一个表达，一个**表达**（*译者注：utterance，以下均使用 utterance*）是一个句子的口语化版本：

> I do uh main- mainly business data processing

这个 utterance 有两种类型的**卡顿**（*译者注：disfluency，或者叫不流利*）。*main-* 这种断开的词叫做**片段**（*fragment*）。像 *uh* 和 *um* 这样的词叫**填充词**（*fillers*）或**填充停顿**（*filled pauses*）。我们是否应该认为这些也是词呢？同样，这取决于具体应用。如果我们要建立一个语音转录（*speech transcription*）系统，我们最终可能想要要去掉这种卡顿。

但我们有时也会保留卡顿。其实在语音识别中，像 *uh* 或 *um* 这样的卡顿对预测接下来的词很有帮助，因为它们可能预示着说话人重新组织语言或想法，所以对于语音识别来说，它们会被当作常规单词来对待。由于人们卡顿的习惯不同，所以它们也可以成为识别说话人的线索。事实上 Clark 和 Fox Tree(2002)[^3]表明，*uh* 和 *um* 有不同的含义。你认为它们分别是什么含义？

像 *They* 这样的大写单词和像 *they* 这样的非大写单词是同一个词吗？在某些任务（语音识别）中，这些词被归为一类，而对于词性标注或命名实体识别（*named-entity tagging*）来说，大写作为一个很有用的特征是被保留的。

那像 *cats* 与 *cat* 这样的变形形式（*inflected forms*）呢？这两个词的**词素**（*lemma*）都是 *cat*，但却是不同的词形（*wordform*）。**词素**是一组具有相同词干（*stem*）、相同主词性和相同词义的词汇形式（*lexical form*）。词形是该词的完整变形或派生（*derived*）形式。对于像阿拉伯语这样形态复杂（*morphologically complex*）的语言，我们经常需要进行词形还原（*lemmatization*）。然而对于英语中的许多任务，词形就足够了。

英语有多少个单词？要回答这个问题，我们需要区分词型（*type*）和词例（*token*）。Type 是指语料库中不同单词的数量，如果用 $V$ 来表示词汇表中词的集合，那么 type 的数量就是词汇表大小 $|V|$。Token 是指所有单词总数 $N$。如果我们忽略标点符号，那么下面的 Brown 句子有 16 个 token，14 个 type（*译者注：即 `n_token = len(sentence.split())`，`n_type = len(set(len(sentence.split())))`，注意去掉标点符号*）：

> They picnicked by the pool, then lay back on the grass and looked at the stars.

当我们说的一个语言中词的数量时，一般指的是词型 type 的数量。

![Fig 2.11](assets/fig2.11.png)

图 2.11 显示了一些常见英语语料库中大致的 type 数和 token 数。语料库越大，type 数越多，事实上，type 数 $|V|$ 和 token 数 $N$ 之间的这种关系被称为 **Herdan 法则**（*Herdan’s Law*）（Herdan，1960）[^4] 或 **Heaps 法则**（*Heaps’ Law*）（Heaps，1978）[^5] ，以其发现者的名字命名（分别在语言学和信息检索领域），如式 2.1，其中 $k$ 和 $\beta$ 为正常数，且 $0 \lt \beta \lt 1$。

$$|V| = kN^\beta \tag{2.1}$$

$\beta$ 的值取决于语料库大小和体裁。对于图 2.11 中的大型语料库来说，$\beta$ 的范围在 0.67 到 0.75 之间。那么大致上我们可以说，一个文本的词汇表大小上升的速度明显快于其 token 数的平方根。

衡量一个语言中单词数量的另一个标准是词素 lemma 的数量，而不是词形 type 的数量。词典可以大致给出 lemma 的数量；词典条目或黑体字形式是 lemma 数量的一个非常粗略的上限（因为有些 lemma 有多种黑体字形式）。1989 年版《牛津英语词典》（*Oxford English Dictionary*）有 615,000 个词条。

[^1]: Kucera, H. and Francis, W. N. (1967). Computational Analysis of Present-Day American English. Brown University Press, Providence, RI.  
[^2]: Godfrey, J., Holliman, E., and McDaniel, J. (1992). SWITCHBOARD: Telephone speech corpus for research and development. ICASSP.  
[^3]: Clark, H. H. and Fox Tree, J. E. (2002). Using uh and um in spontaneous speaking. Cognition 84, 73–111.  
[^4]: Herdan, G. (1960). Type-token mathematics. Mouton.  
[^5]: Heaps, H. S. (1978). Information retrieval. Computational and theoretical aspects. Academic Press.  

