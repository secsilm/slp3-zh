## 8.2 词性标注（*Part-of-Speech Tagging*）

**词性标注**（*Part-of-speech tagging*）是为文本中的每个单词分配一个词性的过程。输入是一个（分词后的）词序列 $x_1,x_2,\ldots,x_n$ 和一个标签集，输出为标签序列 $y_1,y_2,\ldots,y_n$，每个输出 $y_i$ 对应于一个输入 $x_i$，如图 8.3 所示。

![图 8.3](assets/fig8.3.png)

标注是一项**消歧**（*disambiguation*）任务；单词的意义是有歧义的 —— 可能有一个以上的词性，目标是找到适合当前语境的标注。例如，*book* 可以是一个动词（*book that flight*）或一个名词（*hand me that book*）。*That* 可以是一个定语（*Does that flight serve dinner*），也可以是一个补语连词（*I thought that your flight was earlier*）。词性标注的目标是**解析**（*resolve*）这些歧义，为当前语境选择一个适当的标注。

词性标注算法的**准确率**（*accuracy*）（在测试集上与人类标注匹配的百分比）非常高。一项研究发现，Universal Dependency（UD）树库（*treebank*）在 15 种语言中的准确率超过 97% (Wu and Dredze, 2019)[^1]。其他各种英语树库的准确度也为 97%（无论用什么算法；HMM、CRF、BERT 的表现都相似）。这个 97% 也是人类在此任务上的水平，至少对于英语而言 (Manning, 2011)[^2]。

> 译者注：关于上面说的 15 种语言在 UD 上的准确率超过 97%：
>
> 1. 这是一个平均结果，其中德语和荷兰语（*nl*，*Dutch*）的结果是低于 97% 的。
> 2. 汉语不在这 15 种语言中。
> 
> ![15 种语言在 UD 上的测试结果](assets/pos-tagging-test.png)

我们将在接下来的几节中介绍该任务的算法，但首先让我们探讨一下这项任务。它到底有多难？图 8.4 显示，大多数词型（*word types*）（85 - 86%）是很确定的（*Janet* 总是 NNP，*Husitantly* 总是 RB）。但是，这些有歧义的词，虽然只占词汇表的 14 - 15%，但却非常普遍，实际文本（*running text*）中 55 - 67% 的 token 是有歧义的。常见的有歧义的词包括 *that*、*back*、*down*、*put* 和 *set*；以下是一些关于 *back* 这个词的 6 个不同词性的例子：

> earnings growth took a **back/JJ** seat  
> a small building in the **back/NN**  
> a clear majority of senators **back/VBP** the bill  
> Dave began to **back/VB** toward the door  
> enable the country to buy **back/RP** debt  
> I was twenty-one **back/RB** then

![图 8.4](assets/fig8.4.png)

尽管如此，许多词的词性还是很容易辨别的，因为它们的每个标签的可能性并不一样。例如，*a* 可以是一个定语，也可以是字母 *a*，但定语的可能性更大。

这个想法实际上是一个有用的**基准**（*baseline*）：给定一个有歧义的词，选择训练语料库中**出现次数最多**的那个标签。这是一个关键概念：

> **最频繁类基准**（*Most Frequent Class Baseline*）：始终将分类器与基准进行比较，至少要与最频繁类基准一样好（将每个 token 分配给它在训练集中最常出现的类别）。

最频繁类基准的准确率约为 92%[^3]。因此，该基准与最先进的人类的上限（97%）仅相差 5%。

[^1]: Wu, S. and Dredze, M. (2019). Beto, Bentz, Becas: The surprising cross-lingual effectiveness of BERT. EMNLP.  
[^2]: Manning, C. D. (2011). Part-of-speech tagging from 97% to 100%: Is it time for some linguistics?. CICLing 2011.  
[^3]: In English, on the WSJ corpus, tested on sections 22-24.  
