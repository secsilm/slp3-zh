## 8.4 HMM 词性标注（*HMM Part-of-Speech Tagging*）

在这一节中，我们将会介绍我们的第一个序列标注算法，即隐马尔可夫模型（*Hidden Markov Model*），并展示如何将其应用于词性标注。简单回顾一下，一个序列标注器的任务是为序列中的每个单元分配一个标签，从而将一个观察序列（*sequence of observations*）映射到一个相同长度的标签序列。HMM 是一个经典模型，它引入了序列建模的许多关键概念，我们将在更多现代模型中再次看到这些概念。

HMM 是一个概率序列模型：给定一个单元序列（单词、字母、语素和句子等任何东西），它会计算所有可能的标签序列服从的概率分布，并选择最佳的那个标签序列。

### 8.4.1 马克可夫链（*Markov Chains*）

HMM 是一种加强版的马尔可夫链。**马尔可夫链**（*Markov chain*）模型可以告诉我们关于随机变量或者随机*状态*（*states*）序列的概率，其中每一个值都可以从一些集合中取。这些集合可以是词、标签，或者代表任何东西的符号，例如天气。马尔可夫链做了一个非常强的假设，即如果我们想预测该序列未来的某个状态，那么唯一重要的是当前状态。在当前状态之前的所有状态都对未来没有影响，只有当前状态会影响未来状态。这就好像要预测明天的天气一样，你可以根据今天的天气来预测，但你不能根据昨天的天气。

更正式地说，考虑一个状态变量序列 $q_1,q_2,\ldots,q_i$。马尔可夫模型关于这个序列的概率的**马尔科夫假设**（*Markov assumption*）体现在：当预测未来时，过去并不重要，只有现在才重要。

$$马尔可夫假设：P(q_i = a | q_1 \ldots q_{i-1}) = P(q_i = a | q_{i-1}) \tag{8.3}$$

图 8.8a 显示了一个马尔可夫链，用于为一个天气事件序列分配概率，其中的词汇表包括 `HOT`、`COLD` 和 `WARM`。图中的节点表示状态，而边表示状态转移及其概率。转移是一种概率：从一个状态出发的所有弧线的值的和必须为 1。图 8.8b 显示了一个用于给词序列 $w_1, \ldots, w_t$ 分配概率的马尔可夫链。这个马尔可夫链想必已经很熟悉了；事实上，它表示的是一个二元语言模型（*bigram language model*），每条边表示概率 $p(w_i|w_j)$。根据图 8.8 中的两个模型，我们可以给我们词汇表中的任何序列分配一个概率。

![图 8.8](assets/fig8.8.png)

从形式上看，马尔可夫链是由以下部分组成的：

| $Q = q_1 q_2 \ldots q_N$ | $N$ 个**状态**（*states*）的集合 |
|---|---|
| $A = a_{11} a_{12} \ldots a_{N1} \ldots a_{NN}$ | 转移概率矩阵（*transition probability matrix*）$A$，每一个 $a_{ij}$ 表示从状态 $i$ 转移到状态 {j} 的概率，s.t. $\Sigma_{j=1}^{n} a_{ij} \; \forall{i}$ |
| $\pi = \pi_1, \pi_2, \ldots, \pi_N$ | 状态所服从的**初始概率分布**（*initial probability distribution*），$\pi_i$ 表示马尔可夫链从状态 {i} 开始的概率。一些状态 $j$ 可能有 $\pi_j = 0$，意味着它们不可能是初始状态。另外，$\Sigma_{i=1}^{N} \pi_i = 1$|

在继续之前，请使用图 8.8a 中的样本概率（$\pi = [0.1, 0.7, 0.2]$）来计算以下每个序列的概率：

```
(8.4)
hot hot hot hot

(8.5)
cold hot cold hot
```

这些概率的差异告诉了你一个被编码在图 8.8a 中的关于真实世界的天气事实，那么这个事实是什么？

### 8.4.2 隐马尔可夫模型（*The Hidden Markov Model*）

当我们需要计算一个观察事件序列的概率时，马尔可夫链是很有用的。然而，在许多情况下，我们感兴趣的事件是**隐藏的**（*hidden*）：我们并不能直接观察它们。例如，我们不能直接观察到文本中的词性标签。相反，我们看到的是单词，并且必须从单词序列中推断出标签。我们称这些标签为**隐藏的**，因为它们没有被观察到。

**隐马尔可夫模型**（*hidden Markov model*）（HMM）同时支持*可观察*事件（如我们在输入中所看到的词）和我们认为在概率模型中具有因果关系的*隐藏*事件（如词性标签）。HMM 是由以下几个部分组成的：

| $Q = q_1 q_2 \ldots q_N$ | $N$ 个**状态**（*states*）的集合 |
|---|---|
| $A = a_{11} a_{12} \ldots a_{N1} \ldots a_{NN}$ | **转移概率矩阵**（*transition probability matrix*）$A$，每一个 $a_{ij}$ 表示从状态 $i$ 转移到状态 {j} 的概率，s.t. $\Sigma_{j=1}^{n} a_{ij} \; \forall{i}$ |
| $O = o_1 o_2 \ldots o_T$ | 由 $T$ 个**观察值**组成的序列，每个观察值都来自一个词汇表 $V = v_1, v_2, \ldots, v_V$ |
| $B = b_i(o_t)$ | **观察值似然**（*likelihoods*）序列，也称为**发射概率**（*emission probabilities*），每一个表示从状态 $q_i$ 产生观察值 $o_t$ 的概率 |
| $\pi = \pi_1, \pi_2, \ldots, \pi_N$ | 状态所服从的**初始概率分布**（*initial probability distribution*），$\pi_i$ 表示马尔可夫链从状态 {i} 开始的概率。一些状态 $j$ 可能有 $\pi_j = 0$，意味着它们不可能是初始状态。另外，$\Sigma_{i=1}^{N} \pi_i = 1$|

一阶隐马尔可夫模型包含两个简化的假设。首先，与一阶马尔可夫链一样，一个特定状态的概率只取决于前一个状态：

$$马尔可夫假设：P(q_i | q_1 \ldots q_{i-1}) = P(q_i | q_{i-1}) \tag{8.6}$$

其次，一个输出观察值 $o_i$ 的概率只取决于产生该观察值的状态 $q_i$ ，而不取决于任何其他状态和其他观察值：

$$输出独立：P(o_i | q_1, \ldots, q_i, \ldots, q_T, o_1, \ldots, o_i, \ldots, o_T) = P(o_i | q_i) \tag{8.7}$$

### 8.4.3 HMM 标注器组件（*The components of an HMM tagger*）

让我们先看看 HMM 标注器的组成部分，然后再看如何用它来进行标注。一个 HMM 有两个组成部分，即概率 $A$ 和 $B$。

矩阵 $A$ 包含标签转移概率 $P(t_i|t_{i-1})$，表示在给定前一个标签的情况下，出现当前标签的概率。例如，情态动词后面很可能是一个基本形式的动词，即 VB，如 *race*，所以我们会期望这种概率会很高。我们通过统计一个标注语料库中第一个和第二个标签的出现次数，来计算这个转移概率的最大似然估计：

$$P(t_i|t_{i-1}) = \dfrac{C(t_{i-1}, t_i)}{C(t_{i-1})} \tag{8.8}$$

例如，在 WSJ 语料库中，MD 出现了 13124 次，其中后面跟着 VB 的情况出现了 10471 次，所以其 MLE 估计为

$$P(VB|MD) = \dfrac{C(MD, VB)}{C(MD)}=\dfrac{10471}{13124}=0.80 \tag{8.9}$$

让我们先通过一个标注任务例子看看这些概率是如何被估计和使用的，然后我们再回到解码算法。

在 HMM 标注中，概率是通过统计标注训练语料库来估计的。在这个例子中，我们将使用标注好的 WSJ 语料库。

发射概率 $B$，$P(w_i|t_i)$，表示给定一个标签（例如 MD），其对应词是某个给定词（如 *will*）的概率。该发射概率的 MLE 是

$$P(w_i|t_i) = \dfrac{C(t_i, w_i)}{C(t_i)} \tag{8.10}$$

在 WSJ 语料库中出现的 13124 次 MD 中，其对应词是 *will* 的有 4046次：

$$P(will|MD) = \dfrac{C(MD, will)}{C(MD)}=\dfrac{4046}{13124}=0.31 \tag{8.11}$$

我们在第四章中见过这种贝叶斯模型；请记住，这个似然项不是在问“*will* 这个词最可能的标签是什么？” 那将是后验概率 $P(MD|will)$。相反，$P(will|MD)$ 回答的是一个略显反常的问题：“如果我们要生成一个 MD，那么这个情态动词有多大可能是 *will*？”

图 8.9 表示有着 3 个状态（*译者注：VB、MD、NN，即词性标签，观察值 $o_i$ 指的是词，如 will*）的 HMM 标注器，解释了 HMM 的转移概率 $A$和观察值似然 $B$。完整的标注器对每个标签都有一个状态。

![图 8.9](assets/fig8.9.png)

### 8.4.4 HMM 标注的解码部分（*HMM tagging as decoding*）

对于任何包含隐变量的模型，如 HMM，确定与观察值序列相对应的隐变量序列的任务被称为**解码**（*decoding*）。更正式来说：

> 解码：给定一个 HMM $\lambda = (A, B)$ 和一个观察值序列 $O = o_1, o_2, \ldots, o_T$ 作为输入，找出最有可能的状态序列 $Q = q_1 q_2 q_3 \ldots q_T$。

对于词性标注，HMM 解码的目标给定 $n$ 个词的序列 $w_1 \ldots w_n$，选择最有可能的标签序列 $t_1 \ldots t_n$：

$$\hat{t}_{1:n} = \argmax_{t_1 \ldots t_n} P(t_1 \ldots t_n | w_1 \ldots w_n) \tag{8.12}$$

我们在 HMM 中实际会使用贝叶斯法则来计算：

$$\hat{t}_{1:n} = \argmax_{t_1 \ldots t_n} \dfrac{P(w_1 \ldots w_n | t_1 \ldots t_n) P(t_1 \ldots t_n)}{P(w_1 \ldots w_n)} \tag{8.13}$$

此外，我们继续简化公式 8.13，直接去掉分母 $P(w_1^n)$：

$$\hat{t}_{1:n} = \argmax_{t_1 \ldots t_n} P(w_1 \ldots w_n | t_1 \ldots t_n) P(t_1 \ldots t_n) \tag{8.14}$$

HMM 标注器又做了两个简化假设。第一个假设是，一个词出现的概率只取决于它自己的标签，而与邻近的词和标签都无关：

$$P(w_1 \ldots w_n | t_1 \ldots t_n) \approx  \prod_{i=1}^n P(w_i | t_i) \tag{8.15}$$

第二个假设，即 bigram 二元假设，指一个标签的概率只取决于前一个标签，而不是整个标签序列：

$$P(t_1 \ldots t_n) \approx \prod_{i=1}^n P(t_i | t_{i-1}) \tag{8.16}$$

将公式 8.15 和公式 8.16 代入公式 8.14 中，得到以下用于计算二元标注器下最有可能的标签序列的公式：

$$\hat{t}_{1:n} = \argmax_{t_1 \ldots t_n} P(t_1 \ldots t_n | w_1 \ldots w_n) \approx \argmax_{t_1 \ldots t_n} \prod_{i=1}^n \overbrace{P(w_i|t_i)}^{发射概率} \overbrace{P(t_i|t_{i-1})}^{转移概率} \tag{8.17}$$

公式 8.17 的两部分正好对应于我们刚才定义的**发射概率** $B$ 和**转移概率** $A$！

### 8.4.5 维特比算法（*The Viterbi Algorithm*）

HMM 的解码算法是如图 8.10 所示的维特比（*Viterbi*）算法。作为动态规划的一个例子，Viterbi 算法类似于第二章的动态规划最小编辑距离算法。

![图 8.10](assets/fig8.10.png)

Viterbi 算法首先建立一个概率矩阵或者叫**网格**（*lattice*），其中每列为每个观察值 $o_t$，每行为状态图中的每个状态。因此，在单个组合自动机（*single combined automaton*）中，每列（*观察值*）对于每个状态 $q_i$ 都有一个单元格。图 8.11 显示了 *Janet will back the bill* 这个句子对应的 lattice。

![图 8.11](assets/fig8.11.png)
